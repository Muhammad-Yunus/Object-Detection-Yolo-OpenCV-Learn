{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pertemuan 2\n",
    "- Intro to Object Detection Algorithm\n",
    "- Intro to Yolo Model\n",
    "- Model Inferencing using OpenCV DNN (Darknet Yolo & PyTorch Yolo)\n",
    "- Model Inferencing in Grove Vision AI Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Object Detection Algorithm\n",
    "- Object detection is the task of `detecting instances` of objects of a certain class within an image. ([paperswithcode.com](https://paperswithcode.com/task/object-detection))\n",
    "![](resource/object-detection-prev.gif)\n",
    "- The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. \n",
    "    - `One-stage methods` prioritize inference `speed`, and example models include `YOLO`, `SSD` and `RetinaNet`. \n",
    "    - `Two-stage methods` prioritize detection `accuracy`, and example models include `Faster R-CNN`, `Mask R-CNN` and `Cascade R-CNN`.<br><img src=\"resource/Object_detection-Type.png\" width=\"400px\">\n",
    "- `Object Detection` VS `Image Classification` :\n",
    "    - `Image Clasification` : classify an image into a certain category. \n",
    "    - `Object Detection` : Identify the `location` of objects in an image.<br>\n",
    "    <img src=\"resource/objectdetection-vs.jpg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Evaluation metric\n",
    "- To determine and compare the predictive performance of different object detection models, we need standard quantitative metrics.<br><br>\n",
    "- **mAP**\n",
    "    - The most common evaluation metric is **mAP** (`mean average precision`) in Object detection (localisation and classification).\n",
    "    - mAP is the average of **AP** (`average precision`).<br>\n",
    "    <img src=\"resource/mAP-2.png\" width=\"600px\"><br>\n",
    "- **AP**\n",
    "    - Average Precision (AP) is finding the area `under` the `precision-recall` curve.<br><br><br><br>\n",
    "- How to calculate `AP` and `mAP` ?<br><br>\n",
    "\n",
    "    - Understanding **predicted** vs **ground truth** bounding box\n",
    "        - For each bounding box, we have the `predicted` bounding box and the `ground truth` bounding box. <br>\n",
    "        <img src=\"resource/obj-box.png\" width=400px><br><br>\n",
    "\n",
    "\n",
    "    - **IoU**\n",
    "        - Then measure an `overlap` between the `predicted` bounding box and the `ground truth` bounding box, this is called **IoU** (`intersection over union`).<br>\n",
    "    <img src=\"resource/iou.png\" width=\"700px\"><br><br>\n",
    "\n",
    "    - IoU Threshold of **True Positive (TF)** vs **False Negative (FN)** vs **False Positive (FP)**\n",
    "        - **TP** : The model predicted that a bounding box exists at a certain position (positive) and it was correct (true)\n",
    "        - **FP** : The model predicted that a bounding box exists at a particular position (positive) but it was wrong (false)\n",
    "        - **FN** : The model did not predict a bounding box at a certain position (negative) and it was wrong (false)\n",
    "        - Find **TF**, **FN** and **FP** by applying some IoU threshold (usually `0.5`).<br>\n",
    "        - For example, \n",
    "            - if IoU value for a prediction is `0.96`, then we classify it as `True Positive (TF)`. \n",
    "            - On the other hand, if IoU is `0.22`, we classify it as `False Positive (FP)`.\n",
    "            - Remember, if IoU is `0.00`, we classify it as `False Negative (FN)`, \n",
    "                - indicating no intersection between predicted and ground truth box.<br>\n",
    "            <img src=\"resource/iou-2.png\" width=\"800px\"><br><br>\n",
    "    - Calculate Precision & Recall for different threshold values<br>\n",
    "        <img src=\"resource/prec-rec.png\" width=\"500px\"><br><br>\n",
    "\n",
    "    - plot precision & recall curve and calculate area under the curve,<br>\n",
    "        - a precision-recall curve plots the value of precision against recall for different threshold values.\n",
    "        - sort from high to low score of precision vs recall<br>\n",
    "        <img src=\"resource/ap.png\" width=\"500px\">\n",
    "    - calculate **mAP** by averaging all **AP** result for each class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
