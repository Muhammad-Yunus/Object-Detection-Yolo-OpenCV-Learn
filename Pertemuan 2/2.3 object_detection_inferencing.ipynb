{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Object Detection Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Machine Learning vs Deep Learning\n",
    "- *Machine learning and deep learning are both types of AI. In short, **machine learning** is AI that can automatically adapt with minimal human interference. **Deep learning** is a subset of machine learning that uses artificial **neural networks** to mimic the learning process of the human brain.* â€” [coursera.org](https://www.coursera.org/articles/ai-vs-deep-learning-vs-machine-learning-beginners-guide)<br><br>\n",
    "<img src=\"resource/ml.png\" style=\"width:500px\"></img><br><br>\n",
    "\n",
    "- Inference/Forward Pass comparison :<br>\n",
    "<img src=\"resource/ml-dl.png\" style=\"width:700px;background:white\"></img><br>\n",
    "    - **ML** memisahkan bagian **Feature Extraction** dengan bagian **Classification/Detection/Prediction**.\n",
    "    - **DL** menggabungkan bagian **Feature Extraction** dengan bagian **Classification/Detection/Prediction** dalam satu model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training vs Inferencing Deep Learning Model\n",
    "\n",
    "- **Training** memerlukan **dataset** dan akan menghasilkan **Model**.\n",
    "- **Inference** memerlukan **Model** dan **data test** yang akan menghasilkan **Prediction**.  \n",
    "<img src=\"resource/training-inferencing.jpg\" style=\"width:700px\"></img><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How to Training Deep Learning Model\n",
    "\n",
    "- Untuk mentraining Model Deep Learning bisa menggunakan **Framework Deep Learning** yang sudah standard, seperti :\n",
    "    - [Tensorflow](https://www.tensorflow.org/)\n",
    "    - [Keras](https://keras.io/)\n",
    "    - [MXNet](https://mxnet.apache.org/)\n",
    "    - [Caffe](https://caffe.berkeleyvision.org/)\n",
    "    - [H2O](https://h2o.ai/)\n",
    "    - [Darknet](https://github.com/pjreddie/darknet)\n",
    "    - [Torch](https://pytorch.org/)\n",
    "<br><br>\n",
    "- Pelajari Selengkapnya terkait **Training Model Deep Learning** :\n",
    "    - [[Notebook] Intro & Training Model Face Recognition with Keras](https://github.com/Muhammad-Yunus/Materi-Training/blob/main/C.%20Facerecognition/pertemuan_7/2.%20Implementasi%20Neural%20Network.ipynb)\n",
    "    - [[Video] Intro to Neural Network with Keras](https://www.youtube.com/watch?v=Vt8oYlwHYIE&t=3047s)\n",
    "    - [[Video] Training Model Face Recognition with Keras](https://www.youtube.com/watch?v=m0OWRRGYZx8&t=2021s)\n",
    "    - [[Notebook] Intro to Object Detection with Tensorflow](https://github.com/Muhammad-Yunus/Jetson-Nano-Object-Detection-Learn/tree/main/pertemuan_2)\n",
    "    - [[Notebook] Training Model Object Detection with Tensorflow](https://github.com/Muhammad-Yunus/Jetson-Nano-Object-Detection-Learn/tree/main/pertemuan_3)\n",
    "    - [[Video] Training Model Object Detection with Tensorflow](https://www.youtube.com/watch?v=utRrw1TJG-U&t=3808s)\n",
    "    - [[Notebook] Intro & Training Yolo Model Object Detection with Darknet](https://github.com/Muhammad-Yunus/Jetson-Nano-Object-Detection-Learn/tree/main/pertemuan_4)\n",
    "    - [[Video] Intro & Training Yolo Model Object Detection with Darknet](https://www.youtube.com/watch?v=kb2nsM8EN0M&t=42s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimana kita dapat mendownload model yang sudah di training?\n",
    "    - [OpenCV Zoo](https://github.com/opencv/opencv_zoo)\n",
    "    - [https://modelzoo.co/](https://modelzoo.co/)\n",
    "    - [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/archive/research/object_detection/g3doc/tf1_detection_zoo.md)\n",
    "    - [ONNX Model Zoo](https://github.com/onnx/models)\n",
    "    - [Darknet Yolo](https://pjreddie.com/darknet/yolo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 OpenCV DNN\n",
    "\n",
    "**OpenCV DNN - Deep Neural Network** adalah library untuk **Inference** atau **Forward Pass** Model Deep Learning dari beragam framework populer. Menyediakan struktur prrogram yang sederhana dan high performance (mensupport beragam CPU,GPU dan Inference Engine).\n",
    "- Compatibility : > OpenCV 3.3\n",
    "- Wiki : https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "- The supported frameworks:\n",
    "    - Caffe\n",
    "    - TensorFlow\n",
    "    - Torch\n",
    "    - Darknet (Yolo)\n",
    "    - Models in ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Deep Learning Model using OpenCV DNN\n",
    "    - `cv2.dnn.readNet(model, configration)` \n",
    "    - where :\n",
    "        - `model` :\n",
    "            - `*.caffemodel` (Caffe, http://caffe.berkeleyvision.org/)\n",
    "            - `*.pb` (TensorFlow, https://www.tensorflow.org/)\n",
    "            - `*.t7` | `*.net` (Torch, http://torch.ch/)\n",
    "            - `*.weights` (Darknet, https://pjreddie.com/darknet/)\n",
    "        - `configuration` :\n",
    "            - `*.prototxt` (Caffe, http://caffe.berkeleyvision.org/)\n",
    "            - `*.pbtxt` (TensorFlow, https://www.tensorflow.org/)\n",
    "            - `*.cfg` (Darknet, https://pjreddie.com/darknet/)\n",
    "    - This function automatically detects an origin framework of trained model and calls an appropriate function such \n",
    "        - `cv2.dnn.readNetFromCaffe` \n",
    "        - `cv2.dnn.readNetFromTensorflow`\n",
    "        - `cv2.dnn.readNetFromTorch` \n",
    "        - `cv2.dnn.readNetFromDarknet`\n",
    "    - OpenCV DNN config file bisa ditemukan [disini](https://github.com/opencv/opencv_extra/tree/4.x/testdata/dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Inferencing Yolo - Darknet Model using OpenCV DNN\n",
    "- Load Model **Yolo v3 Tiny**\n",
    "    - Download model [yolov3-tiny.weights](https://pjreddie.com/media/files/yolov3-tiny.weights)\n",
    "    - Lalu download juga config [yolov3-tiny.cfg](https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3-tiny.cfg),\n",
    "    - Setelahnya masukan kedua file tersebut ke folder `/Pertemuan 3/model/`<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model `Yolo v3 Tiny` di training dengan `COCO Dataset` yang terdiri dari `80 class names` dalam `80 class index`\n",
    "- load yolo coco class names via `.load_coco_class_names_yolo()` in `coco.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coco\n",
    "\n",
    "classNames = coco.load_coco_class_names_yolo()\n",
    "\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load yolo model\n",
    "    - pastikan `yolov3-tiny.weights` dan `yolov3-tiny.cfg` sudah ada dalam folder `model/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"model/yolov3-tiny.weights\"\n",
    "config = \"model/yolov3-tiny.cfg\"\n",
    "net = cv2.dnn.readNet(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load image and convert to blob with `scaleFactor=1/255.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"image1.jpg\")\n",
    "\n",
    "resize_h, resize_w = 640, 640 \n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1/255.0, (resize_w, resize_h), (0, 0, 0), swapRB=True, crop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do a net forward (inferencing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blob)\n",
    "layerOutput = net.getUnconnectedOutLayersNames()\n",
    "output = net.forward(layerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Postprocessing detection result via `utils.py`\n",
    "    - Apply [NMS Box](https://learnopencv.com/tag/cv-dnn-nmsboxes/)\n",
    "    - Draw Detection Box\n",
    "    - use `.postprocess` to do postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load utils.py\n",
    "\n",
    "import utils \n",
    "\n",
    "utility = utils.Utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do a postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = utility.postprocess_darknet(output, img, classNames, confThreshold = 0.5, nmsThreshold = 0.3, font_size=0.5, \n",
    "                        color=(255,127,0), text_color=(255,255,255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- show result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"detection result\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inferencing on video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"model/yolov3-tiny.weights\"\n",
    "config = \"model/yolov3-tiny.cfg\"\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "# load video\n",
    "cap = cv2.VideoCapture(\"video.mp4\")\n",
    "           \n",
    "# iterate for each frame in video\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # get image on each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # convert to blob\n",
    "    resize_h, resize_w = 320, 320 # use smaller image size to reduce computation \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (resize_w, resize_h), (0, 0, 0), swapRB=True, crop=True)\n",
    "\n",
    "    # do a net forward (inferencing)\n",
    "    net.setInput(blob)\n",
    "    layerOutput = net.getUnconnectedOutLayersNames()\n",
    "    output = net.forward(layerOutput)\n",
    "\n",
    "    # do postprocessing\n",
    "    frame = utility.postprocess_darknet(output, frame, classNames, confThreshold = 0.3, nmsThreshold = 0.2, font_size=0.5, \n",
    "                        color=(255,127,0), text_color=(255,255,255))\n",
    "\n",
    "    # show result\n",
    "    cv2.imshow('Frame',frame)\n",
    "\n",
    "    # wait 25ms per frame and close using 'q' \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "          break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Inferencing Yolo - PyTorch Model using OpenCV DNN\n",
    "- To use Pytorch Model in OpenCV DNN we need to convert the Pytorch model data type `.pt` into ONNX format.\n",
    "- We will use [Convert Pytorch Model (.pt) to ONNX.ipynb](https://colab.research.google.com/drive/1IDHaSJyIauPgI_TE9UXHLm2m2nUUaKXb) Notebook file in [Google Colab](https://colab.research.google.com/)\n",
    "- On that notebook we will download Pytorch YOLOv3 tiny model (`yolov3-tinyu.pt`) then converting it into ONNX format `yolov3-tinyu.onnx`\n",
    "- Don't forget to put the downloaded file (`.onnx`) into the `model/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = \"model/yolov3-tinyu.onnx\"\n",
    "net = cv2.dnn.readNetFromONNX(model)\n",
    "\n",
    "# load image convert to blob\n",
    "img = cv2.imread(\"image1.jpg\")\n",
    "resize_h, resize_w = 320, 320 \n",
    "blob = cv2.dnn.blobFromImage(img, 1/255.0, (resize_w, resize_h), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "# do a net forward (inferencing)\n",
    "net.setInput(blob)\n",
    "output = net.forward()\n",
    "\n",
    "# do a postprocessing\n",
    "img = utility.postprocess_onnx(output, img, classNames, confThreshold = 0.5, nmsThreshold = 0.3, font_size=0.5, \n",
    "                        color=(255,127,0), text_color=(255,255,255), input_size=[resize_h, resize_w])\n",
    "\n",
    "# show result\n",
    "cv2.imshow(\"detection result\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inverencing from video file for Yolo - Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = \"model/yolov3-tinyu.onnx\"\n",
    "net = cv2.dnn.readNetFromONNX(model)\n",
    "\n",
    "# load video\n",
    "cap = cv2.VideoCapture(\"video.mp4\")\n",
    "           \n",
    "# iterate for each frame in video\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # get image on each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    h, w, c = frame.shape\n",
    "    resize_h, resize_w = 320, 320\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (resize_w, resize_h), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "    # do a net forward (inferencing)\n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "\n",
    "    # do a postprocessing\n",
    "    frame = utility.postprocess_onnx(output, frame, classNames, confThreshold = 0.5, nmsThreshold = 0.3, font_size=0.5, \n",
    "                            color=(255,127,0), text_color=(255,255,255), input_size=[resize_h, resize_w])\n",
    "\n",
    "    # show result\n",
    "    cv2.imshow('Frame',frame)\n",
    "\n",
    "    # wait 25ms per frame and close using 'q' \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "          break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Inferencing Object Detection using Grove Vision AI Module V2\n",
    "\n",
    "<img src=\"resource/GVAIM.png\" width=\"600px\"><br>\n",
    "- **Powerful AI Processing Capabilities**: Utilizes WiseEye2 HX6538 processor with a dual-core **Arm Cortex-M55** and integrated **Arm Ethos-U55** neural network unit.\n",
    "- **Versatile AI Model Support**: Easily deploy off-the-shelf or your custom AI models from **SenseCraft AI**, including **Mobilenet V1**, **V2**, **Efficientnet-lite**, **Yolo v5** & **v8**. **TensorFlow** and **PyTorch** frameworks are supported.\n",
    "- **Rich Peripheral Devices**: Includes PDM microphone, SD card slot, Type-C, Grove interface, and other peripherals.\n",
    "- **High Compatibility**: Compatible with **XIAO series**, **Arduino**, **Raspberry Pi**, **ESP dev board**, easy for further development.\n",
    "Fully Open Source: All codes, design files, and schematics available for modification and use.<br>\n",
    "<img src=\"resource/GVAIM-2.png\" width=\"600px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
