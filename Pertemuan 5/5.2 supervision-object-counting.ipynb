{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Supervision Object Counting \n",
    "\n",
    "<img src=\"resource/rf-supervision-banner.png\" width=\"700px\"><br><br>\n",
    "- Computer Vision made easy and resusable.\n",
    "    - Whether you need to load your dataset from your hard drive, \n",
    "    - draw detections on an image or video, \n",
    "    - or count how many detections are in a zone. \n",
    "    - Supervision was designed to be model agnostic. \n",
    "    - Just plug in any classification, detection, or segmentation model. \n",
    "- Website : https://supervision.roboflow.com/latest/\n",
    "- Github : https://github.com/roboflow/supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install Supervision\n",
    "\n",
    "- Since Supervision working in `python3.8`, we need to create new conda environment with name `BelajarSuperVision` using that python version\n",
    "- Open `Anaconda prompt`\n",
    "- create new environment `BelajarSuperVision`\n",
    "    ```\n",
    "    conda create --name BelajarSuperVision python=3.8\n",
    "    ```\n",
    "- activate environment\n",
    "    ```\n",
    "    conda activate BelajarSuperVision\n",
    "    ```\n",
    "- run to install supervision & ultralytics\n",
    "    ```\n",
    "    pip install ipykernel\n",
    "    pip install supervision\n",
    "    pip install ultralytics\n",
    "    pip install onnx --user\n",
    "    pip install onnxruntime\n",
    "    ```\n",
    "- Close VS Code, then reopen it\n",
    "- Open `6.2 supervision-object-counting.ipynb`\n",
    "- Choose `BelajarSuperVision` as python environment<br>\n",
    "<img src=\"resource/sv-image.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if supervision was installed, required version >= 0.16.0\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "print(\"sv\", sv.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Object Counting Polygon Zone using Supervision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Singgle Polygon Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the polygon point here!\n",
    "polygon = np.array([[421, 390], [524, 470], [190, 472], [241, 398]])# CHANGE TO YOUR OWN POLYGON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create supervision PolygonZone for the given polygon point\n",
    "zone = sv.PolygonZone(polygon=polygon)\n",
    "\n",
    "# create Supervision BoxAnnotator() & label_annotator()---> similar to utils.py > postprocessing_onnx()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "# create Supervision PolygonZoneAnnotator() ---> similar to utils.py > draw_object_count()\n",
    "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.WHITE, thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Yolo V8 ONNX using Ultralytics Yolo\n",
    "model = \"model/yolov8s.onnx\"\n",
    "model = YOLO(model, task='detect')\n",
    "\n",
    "# load video mall.mp4\n",
    "cap = cv2.VideoCapture(\"mall.mp4\")\n",
    "           \n",
    "# iterate for each frame in video\n",
    "while cap.isOpened():\n",
    "    # get image on each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # do forward pass (inferencing) yolo v8 onnx\n",
    "    results = model(frame, imgsz=320)[0]\n",
    "\n",
    "    # do postprocess detection result\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[detections.class_id == 0] # filter only class_id = 0 --> 'person'\n",
    "    zone.trigger(detections=detections)\n",
    "\n",
    "    # draw bounding box, label \n",
    "    box_labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(detections['class_name'], detections.confidence)\n",
    "    ]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "    frame = label_annotator.annotate(scene=frame, detections=detections, labels=box_labels)\n",
    "\n",
    "    # draw polygon zone and object count label\n",
    "    object_count_label = f\"count {zone_annotator.zone.current_count}\"\n",
    "    frame = zone_annotator.annotate(scene=frame, label=object_count_label)\n",
    "\n",
    "    # show result\n",
    "    cv2.imshow('Frame',frame)\n",
    "\n",
    "    # wait 1ms per frame and close using 'q'\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Multiple Polygon Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the polygon point here!\n",
    "polygon1 = np.array([[421, 390], [524, 470], [190, 472], [241, 398]]) # CHANGE TO YOUR OWN POLYGON\n",
    "polygon2 = np.array([[35, 684], [118, 578], [649, 569], [822, 693]])  # CHANGE TO YOUR OWN POLYGON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create supervision PolygonZone for the given polygon point\n",
    "zone1 = sv.PolygonZone(polygon=polygon1)\n",
    "zone2 = sv.PolygonZone(polygon=polygon2)\n",
    "\n",
    "# create Supervision BoxAnnotator() & label_annotator()---> similar to utils.py > postprocessing_onnx()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "# create Supervision PolygonZoneAnnotator() ---> similar to utils.py > draw_object_count()\n",
    "zone_annotator1 = sv.PolygonZoneAnnotator(zone=zone1, color=sv.Color.WHITE, thickness=2)\n",
    "zone_annotator2 = sv.PolygonZoneAnnotator(zone=zone2, color=sv.Color.WHITE, thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Yolo V8 ONNX using Ultralytics Yolo\n",
    "model = \"model/yolov8s.onnx\"\n",
    "model = YOLO(model, task='detect')\n",
    "\n",
    "# load video mall.mp4\n",
    "cap = cv2.VideoCapture(\"mall.mp4\")\n",
    "           \n",
    "# iterate for each frame in video\n",
    "while cap.isOpened():\n",
    "    # get image on each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # do forward pass (inferencing) yolo v8 onnx\n",
    "    results = model(frame, imgsz=320)[0]\n",
    "\n",
    "    # do postprocess detection result\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[detections.class_id == 0] # filter only class_id = 0 --> 'person'\n",
    "    zone1.trigger(detections=detections)\n",
    "    zone2.trigger(detections=detections)\n",
    "\n",
    "    # draw bounding box, label \n",
    "    box_labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(detections['class_name'], detections.confidence)\n",
    "    ]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "    frame = label_annotator.annotate(scene=frame, detections=detections, labels=box_labels)\n",
    "\n",
    "    # draw polygon zone and object count label\n",
    "    object_count_label1 = f\"count {zone_annotator1.zone.current_count}\"\n",
    "    frame = zone_annotator1.annotate(scene=frame, label=object_count_label1)\n",
    "\n",
    "    object_count_label2 = f\"count {zone_annotator1.zone.current_count}\"\n",
    "    frame = zone_annotator2.annotate(scene=frame, label=object_count_label2)\n",
    "\n",
    "    # show result\n",
    "    cv2.imshow('Frame',frame)\n",
    "\n",
    "    # wait 1ms per frame and close using 'q'\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cons\n",
    "- Couldn't count multiple class data, all object in polygon area will be counting together as total object in polygon area\n",
    "\n",
    "### Pros\n",
    "- More faster than OpenCV DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Source \n",
    "- https://blog.roboflow.com/how-to-count-objects-in-a-zone/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
