{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Intro to Two Stage Object Detection\n",
    "- Two-stage detectors typically consist of a <font color=\"orange\">region proposal</font> stage followed by a <font color=\"orange\">refinement stage</font>.\n",
    "- Historical timeline of prominent two-stage object detection models, <br><br>\n",
    "<img src=\"resource/two-stage-obj-det-timeline.png\" width=\"100%\"> <br><br>\n",
    "- Historical <font color=\"orange\">innovation</font> in two-stage object detection models, <br><br>\n",
    "<img src=\"resource/two-stage-obj-det-innovation.png\" width=\"100%\"> <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "<br><br><br><br>\n",
    "### 7.1.1 <font color=\"orange\">R-CNN</font> (Regions with Convolutional Neural Networks)\n",
    "- Author : Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik\n",
    "- Release Date : 2014\n",
    "- Innovation : \n",
    "    - Introduced the use of <font color=\"orange\">region proposals</font> followed by CNN-based feature extraction for each region. \n",
    "    - R-CNN extracts around 2,000 region proposals using <font color=\"orange\">selective search</font> and classifies each region with a CNN.\n",
    "- Architecture : \n",
    "    - Consists of three stages: \n",
    "        1. <font color=\"orange\">Region proposal</font> using <font color=\"orange\">selective search</font>, \n",
    "            - convert image into <font color=\"orange\">small segments</font> based on <font color=\"orange\">color</font> similarity and <font color=\"orange\">texture</font>,\n",
    "            - <font color=\"orange\">merge</font> segmented region based on <font color=\"orange\">nearest similarity</font> in color, texture, size, or shape,\n",
    "            - produce list of <font color=\"orange\">proposed regions</font> (box) where objects might be located.<br>\n",
    "            <img src=\"resource/selective_search.png\" width=\"600px\"><br>\n",
    "        2. <font color=\"orange\">Feature extraction</font> for each <font color=\"orange\">wrapped region</font> using a <font color=\"orange\">pre-trained CNN</font> (AlexNet or similar), \n",
    "            - wrapping procedure is necessary to standardize proposed regions dimension for input into the network \n",
    "        3. <font color=\"orange\">SVM classifier</font> to <font color=\"orange\">predict object labels</font> and <font color=\"orange\">Linear regression</font> to <font color=\"orange\">predict object box</font> for each proposal.<br>\n",
    "        <img src=\"resource/r-cnn-arch2.png\" width=\"900px\"><br>\n",
    "- Benchmark : PASCAL VOC 2007 achieved 58.5% mAP.\n",
    "- Paper : ['Rich feature hierarchies for accurate object detection and semantic segmentation' - arxiv.org](https://arxiv.org/pdf/1311.2524)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "### 7.1.2 <font color=\"orange\">Fast R-CNN</font>\n",
    "- Author: Ross Girshick\n",
    "- Release Date: 2015\n",
    "- Innovation: \n",
    "    - Improved efficiency by <font color=\"orange\">sharing computation</font> over all region proposals. \n",
    "    - It introduces the <font color=\"orange\">ROI pooling</font> layer to crop features directly from a <font color=\"orange\">shared feature map</font> rather than processing each region independently.\n",
    "- Architecture: \n",
    "    - Similar to R-CNN but uses a <font color=\"orange\">single forward pass</font> through the CNN to generate feature maps,\n",
    "        - Find region proposal using <font color=\"orange\">selective search</font>,\n",
    "        - Apply forward pass image through the CNN producing <font color=\"orange\">feature map</font>,\n",
    "        - <font color=\"orange\">Projecting region</font> into generated feature maps. <br>\n",
    "        <img src=\"resource/fast r-cnn-arch2.png\" width=\"95%\"><br>\n",
    "    - Then applies <font color=\"orange\">ROI pooling</font> to extract features for each region in <font color=\"orange\">fixed size</font>.\n",
    "        - <font color=\"orange\">Divide</font> the Region into Sub-Regions (Grids),\n",
    "        - Apply <font color=\"orange\">Max-pooling</font> in each grid,\n",
    "        - Producing a smaller, <font color=\"orange\">fixed-size</font> region that preserves important features.\n",
    "            - We are no longer need to wrap the region to standarize regions dimension size.<br>\n",
    "        <img src=\"resource/roi-pooling.png\" width=\"900px\"><br>\n",
    "    - Uses a <font color=\"orange\">Softmax</font> classifier instead of SVM to predict label.\n",
    "- Benchmark: PASCAL VOC 2012 achieved 70% mAP.\n",
    "- Paper : ['Fast R-CNN' - arxiv.org](https://arxiv.org/pdf/1504.08083)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "### 7.1.2 <font color=\"orange\">Faster R-CNN</font>\n",
    "- Author: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun\n",
    "- Release Date: 2016\n",
    "- Innovation: \n",
    "    - Introduced a <font color=\"orange\">Region Proposal Network (RPN)</font> to replace selective search, making the model end-to-end trainable. \n",
    "    - The <font color=\"orange\">RPN</font> shares features with the detection network, improving speed and accuracy.\n",
    "- Architecture: \n",
    "    - Combines <font color=\"orange\">RPN</font> and <font color=\"orange\">Fast R-CNN</font> into a unified network. <br>\n",
    "    <img src=\"resource/faster r-cnn-arch2.png\" width=\"95%\"><br><br><br>\n",
    "    - The <font color=\"orange\">RPN</font> generates <font color=\"orange\">region proposals</font>, which are then refined by the <font color=\"orange\">Fast R-CNN</font> detection head using <font color=\"orange\">ROI pooling</font> and a <font color=\"orange\">fully connected layer</font>.<br>\n",
    "        <img src=\"resource/RPN2.png\" width=\"95%\"><br>\n",
    "        - RPN generate multiple <font color=\"orange\">anchor boxes</font> centered at the current sliding window position,\n",
    "            - <font color=\"orange\">anchor box</font> is <font color=\"orange\">predefined bounding boxes</font> of different <font color=\"orange\">scales</font> and aspect <font color=\"orange\">ratios</font> covering various object shapes and sizes.\n",
    "        - <font color=\"orange\">RPN Head</font> use <font color=\"orange\">convolution operation</font> with 3x3 kernel.\n",
    "            - Then passed through the result to two separate branches ($1Ã—1$ <font color=\"orange\">convolutional layers</font>).\n",
    "                - <font color=\"orange\">Objectness Score Prediction</font> : A <font color=\"orange\">binary classifier</font> outputs an objectness score for each anchor (<font color=\"orange\">object</font> or <font color=\"orange\">background</font>),\n",
    "                - <font color=\"orange\">Bounding Box Regression</font> : A regressor predicts the adjustments (offsets) needed to <font color=\"orange\">refine</font> the anchor boxes to better fit the ground-truth bounding boxes.\n",
    "        - <font color=\"orange\">NMS</font> is applied to <font color=\"orange\">filter</font> out <font color=\"orange\">redundant</font> and <font color=\"orange\">overlapping</font> proposals, leaving only the most confident ones.\n",
    "        \n",
    "        \n",
    "    \n",
    "- Benchmark: COCO 2015, achieving around 42.7% mAP.\n",
    "- Paper : ['Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks' - arxiv.org](https://arxiv.org/pdf/1506.01497)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
