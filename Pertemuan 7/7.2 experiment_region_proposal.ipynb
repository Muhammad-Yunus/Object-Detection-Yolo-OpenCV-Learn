{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOyu47Z_6IXa"
      },
      "source": [
        "## 7.2 Experiment Region Proposal\n",
        "\n",
        "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/7.2%20experiment_region_proposal.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
        "- Click `Connect` button in top right Google Colab notebook,<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu.png?raw=1\" width=\"250px\">\n",
        "- If connecting process completed, it will turn to something look like this<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu-success.png?raw=1\" width=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Check GPU connected into Colab environment is active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXLBu336IXk"
      },
      "source": [
        "_________\n",
        "<br><br><br><br>\n",
        "#### 7.2.1 <font color=\"orange\">Selective Search</font>\n",
        "- Simple implementation Selective Search using OpenCV to perform segmentation based on color similarity\n",
        "- Then producing proposed bounding box as a result countour of segmented image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oH9lxNWM6IXm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Y1H_n_6IXp"
      },
      "source": [
        "- Load the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "olBUa6hw6IXq"
      },
      "outputs": [],
      "source": [
        "# Download an example image from repo\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/raw/main/Pertemuan%207/astronaut.jpg\", \"astronaut.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "hRhZSU4I6IXs",
        "outputId": "d1e4ff36-1e92-43e0-a8de-de9bb98debb9"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(\"astronaut.jpg\")\n",
        "\n",
        "# show image using matplot lib\n",
        "def imshow(image, isBGR=True):\n",
        "    if isBGR :\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysAQLVOe6IXu"
      },
      "source": [
        "- Define function for <font color=\"cyan\">Color Quantization</font> using <font color=\"orange\">K-means clustering</font>,\n",
        "    - Choose Number of <font color=\"orange\">Clusters</font> ($k$),\n",
        "        - For example, if you have a bunch of points and want to split them into 3 groups, you set $k=3$.\n",
        "    - Initialize <font color=\"orange\">Centroids</font>,\n",
        "        - Randomly place $k$ points (called <font color=\"orange\">centroids</font>) in the space where your data points lie.\n",
        "    - Update Centroids,\n",
        "        - For each cluster, calculate the <font color=\"orange\">average</font> of all points assigned to that cluster.\n",
        "        - This <font color=\"orange\">new average</font> becomes the updated <font color=\"orange\">centroid</font> for the cluster.\n",
        "    - <font color=\"orange\">Repeat</font> Until Stable,\n",
        "        - Steps 3 and 4 are repeated until the centroids <font color=\"orange\">no longer change</font> much (or at all) between iterations.\n",
        "        - This means each data point has found its \"best\" cluster.<br><br>\n",
        "        <img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/K-Means.gif?raw=1\" width=\"900px\"><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SQp_5ol6IXw"
      },
      "source": [
        "- Implement <font color=\"orange\">K-Means Clustering</font> using OpenCV `cv2.kmeans()`,\n",
        "    ```\n",
        "    retval, bestLabels, centers = cv2.kmeans(data, K, bestLabels, criteria, attempts, flags)\n",
        "    ```\n",
        "    - where :\n",
        "        - `K` Number of clusters to split the set by.\n",
        "        - `bestLabels` Input/output integer array that stores the cluster indices for every sample.\n",
        "        - `criteria` The algorithm termination criteria, can be the maximum number of iterations and/or the desired accuracy.\n",
        "        - `attempts` to specify the number of times the algorithm is executed using different initial labellings.\n",
        "        - `flags` to set random initial centers in each attempt. can take values of,\n",
        "            - cv2.KMEANS_RANDOM_CENTERS     = 0,\n",
        "            - cv2.KMEANS_PP_CENTERS         = 2,\n",
        "            - cv2.KMEANS_USE_INITIAL_LABELS = 1,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "41ao5hmY6IXx"
      },
      "outputs": [],
      "source": [
        "def color_quantization(image, k=5):\n",
        "    # Convert to Lab color space\n",
        "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
        "\n",
        "    # Reshape the image data to a 2D array of pixels\n",
        "    pixel_values = image_lab.reshape((-1, 3))\n",
        "    pixel_values = np.float32(pixel_values)\n",
        "\n",
        "    # Define criteria and apply KMeans clustering\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.2)\n",
        "    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 15, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "    centers = np.uint8(centers)\n",
        "    quantized_image = centers[labels.flatten()]\n",
        "\n",
        "    # Reshape back to the original image dimensions\n",
        "    quantized_image = quantized_image.reshape(image.shape)\n",
        "    return cv2.cvtColor(quantized_image, cv2.COLOR_Lab2BGR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "m374LgXX6IXz",
        "outputId": "0778f63d-8f18-4e9a-c1b5-ba70a2946f5d"
      },
      "outputs": [],
      "source": [
        "# generate segmented image using k=5\n",
        "segmented_image = color_quantization(image, k=5)\n",
        "\n",
        "# show segmented image\n",
        "imshow(segmented_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWw6DGYF6IX0"
      },
      "source": [
        "- Define function to <font color=\"cyan\">Extract Bounding Box</font> from segmented image\n",
        "    - Convert `segmented_image` to binary image (black & white) using Otsu Thresholding (`cv2.THRESH_OTSU`),\n",
        "    - Find contour on binary image using `cv2.findContours()`,\n",
        "    - Find Bouding Box on detecting contour using `cv2.boundingRect()`\n",
        "    - Filter box size $w*h$ >= $min size$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9dMs2wTu6IX0"
      },
      "outputs": [],
      "source": [
        "def get_bounding_boxes(segmented_image, min_size=200):\n",
        "    # Convert to grayscale and apply binary thresholding\n",
        "    gray = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Otsu's thresholding\n",
        "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Create bounding boxes for contours larger than min_size\n",
        "    boxes = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        if w * h >= min_size:\n",
        "            boxes.append((x, y, w, h))\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teSAwHx26IX1",
        "outputId": "7b40f5db-299b-47b7-ce00-b04af1d18c11"
      },
      "outputs": [],
      "source": [
        "boxes = get_bounding_boxes(segmented_image, min_size=500)\n",
        "\n",
        "print(boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6_yA-FD6IX2"
      },
      "source": [
        "- <font color=\"cyan\">Draw Bounding Box</font> on original image using `cv2.rectangle`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "gPIzXpBy6IX2",
        "outputId": "6fad68d9-8ed7-461a-eb71-2e6c36e3cae1"
      },
      "outputs": [],
      "source": [
        "# define maximun number of box to draw\n",
        "MAX_BOX = 100\n",
        "\n",
        "# draw box in original image\n",
        "image_with_boxes = image.copy()\n",
        "for (x, y, w, h) in boxes[:MAX_BOX]:\n",
        "    cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "# show image with bounding box\n",
        "imshow(image_with_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876wBjvL6IX3"
      },
      "source": [
        "_______\n",
        "<br><br><br><br>\n",
        "### 7.2.2 <font color=\"orange\">RPN</font> (Region Proposal Network)\n",
        "- We will try to implement RPN (Region Proposal Network) in Pytorch refered to the following diagram,<br>\n",
        "<img src=\"resource/RPN2.png\" width=\"95%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load pretrained ResNet-34 from torch.hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained ResNet-34 from torch hub\n",
        "backbone = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "\n",
        "# Remove the fully connected layers, keep convolutional layers only\n",
        "backbone = nn.Sequential(*list(backbone.children())[:-2])  # This keeps layers up to the last Conv layer\n",
        "backbone.eval()  # Set backbone to evaluation mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load Image Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load an example image and preprocess it\n",
        "image_path = 'astronaut.jpg'  # Replace with your image path\n",
        "image = cv2.imread(image_path)\n",
        "H, W = 224, 224  # Resize image to this size\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = cv2.resize(image, (W, H))  # Resize to a suitable input size for ResNet\n",
        "\n",
        "# Convert image to tensor and normalize as required by ResNet\n",
        "input_tensor = torch.tensor(image).float().permute(2, 0, 1).unsqueeze(0)\n",
        "input_tensor = input_tensor / 255.0\n",
        "input_tensor = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        ")(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Forward pass image tensor to CNN Backbone (Resnet-34)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate feature map using ResNet-34\n",
        "with torch.no_grad():  # No need to compute gradients here\n",
        "    feature_map = backbone(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2-1mMTG6IX3"
      },
      "source": [
        "<br><br><br><br>\n",
        "- <font color=\"orange\">RPN Head</font> Implementation in PyTorch\n",
        "    - The <font color=\"orange\">RPN Head</font> typically has a <font color=\"orange\">3x3 convolutional layer</font> to scan over the feature map and two output branches:\n",
        "        - A classification branch (object vs. background) using <font color=\"orange\">1x1 convolutional layer</font>.\n",
        "        - A regression branch (bounding box refinements) using <font color=\"orange\">1x1 convolutional layer</font>.\n",
        "            <img src=\"resource/RPN.png\" width=\"700px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fPCR6dG46IX6"
      },
      "outputs": [],
      "source": [
        "class SimpleRPNHead(nn.Module):\n",
        "    def __init__(self, in_channels=512, num_anchors=9):\n",
        "        super(SimpleRPNHead, self).__init__()\n",
        "\n",
        "        # 3x3 Conv layer for sliding window over the feature map\n",
        "        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        # Classification layer: predict if each anchor is an object or not\n",
        "        # Output 2 scores per anchor: object or background\n",
        "        self.cls_score = nn.Conv2d(512, num_anchors * 2, kernel_size=1)\n",
        "\n",
        "        # Regression layer: predict 4 adjustments for each anchor's bbox (dx, dy, dw, dh)\n",
        "        self.bbox_delta = nn.Conv2d(512, num_anchors * 4, kernel_size=1)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for layer in [self.conv, self.cls_score, self.bbox_delta]:\n",
        "            # initializes the weights of each layer with a normal (Gaussian) distribution\n",
        "            # with a mean of 0 and a standard deviation of 0.01\n",
        "            nn.init.normal_(layer.weight, std=0.01)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared convolutional layer\n",
        "        x = F.relu(self.conv(x))\n",
        "\n",
        "        # Classification score for each anchor (object vs. background)\n",
        "        cls_score = self.cls_score(x)\n",
        "\n",
        "        # Bounding box refinement for each anchor\n",
        "        bbox_delta = self.bbox_delta(x)\n",
        "\n",
        "        # Reshape for output\n",
        "        # cls_score: [batch, num_anchors * 2, H, W] -> [batch, H * W * num_anchors, 2]\n",
        "        cls_score = cls_score.permute(0, 2, 3, 1).contiguous()\n",
        "        cls_score = cls_score.view(cls_score.shape[0], -1, 2)\n",
        "\n",
        "        # bbox_delta: [batch, num_anchors * 4, H, W] -> [batch, H * W * num_anchors, 4]\n",
        "        bbox_delta = bbox_delta.permute(0, 2, 3, 1).contiguous()\n",
        "        bbox_delta = bbox_delta.view(bbox_delta.shape[0], -1, 4)\n",
        "\n",
        "        return cls_score, bbox_delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW5shQHu6IX_"
      },
      "source": [
        "- Initialize RPN Head\n",
        "    - `num_anchors=9` is number of anchor in different `scale` and `aspect ratio`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9vwwzSPq6IYA"
      },
      "outputs": [],
      "source": [
        "# Initialize RPN with input channels matching the ResNet output (512 channels in ResNet-34)\n",
        "rpn_head = SimpleRPNHead(in_channels=512, num_anchors=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Y7IolN6IYB"
      },
      "source": [
        "- Forward pass output feature map from backbone network to RPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khvTVlU56IYB",
        "outputId": "4d803909-ea22-4fdf-955f-0667937d52f1"
      },
      "outputs": [],
      "source": [
        "# Run the feature map through RPN to get region proposals\n",
        "cls_scores, bbox_deltas = rpn_head(feature_map)\n",
        "\n",
        "print(f\"Feature map shape: {feature_map.shape}\")\n",
        "print(f\"Class scores shape: {cls_scores.shape}\")\n",
        "print(f\"Bbox deltas shape: {bbox_deltas.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoSfQ_SG6IYC"
      },
      "source": [
        "<br><br><br><br>\n",
        "- <font color=\"orange\">Implement Anchor Box Generation</font>\n",
        "    - Define anchor boxes of multiple <font color=\"orange\">scales<font> and <font color=\"orange\">aspect ratios</font> for each position in a feature map.<br>\n",
        "    <img src=\"resource/anchor-box-generation.png\" width=\"95%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LLj4flJc6IYC"
      },
      "outputs": [],
      "source": [
        "class SimpleAnchorGenerator:\n",
        "    def __init__(self, sizes=[32, 64, 128], aspect_ratios=[0.5, 1.0, 2.0]):\n",
        "        self.sizes = sizes\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "\n",
        "    def generate_anchors(self, grid_size, stride):\n",
        "        anchors = []\n",
        "        for size in self.sizes:\n",
        "            for aspect_ratio in self.aspect_ratios:\n",
        "                # Compute anchor dimensions\n",
        "                anchor_height = size / np.sqrt(aspect_ratio)\n",
        "                anchor_width = size * np.sqrt(aspect_ratio)\n",
        "\n",
        "                # Generate anchors across the grid\n",
        "                for y in range(grid_size[0]):\n",
        "                    for x in range(grid_size[1]):\n",
        "                        # Convert grid cell coordinates to center coordinates in image space\n",
        "                        center_x = (x + 0.5) * stride\n",
        "                        center_y = (y + 0.5) * stride\n",
        "\n",
        "                        # Calculate the (x_min, y_min, x_max, y_max) for each anchor\n",
        "                        x_min = center_x - 0.5 * anchor_width\n",
        "                        y_min = center_y - 0.5 * anchor_height\n",
        "                        x_max = center_x + 0.5 * anchor_width\n",
        "                        y_max = center_y + 0.5 * anchor_height\n",
        "\n",
        "                        anchors.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        return torch.tensor(anchors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVLqZXFK69op",
        "outputId": "244ec2aa-d626-4eb4-dfff-24c995e610d9"
      },
      "outputs": [],
      "source": [
        "anchor_generator = SimpleAnchorGenerator(sizes=[8, 16, 32], aspect_ratios=[1.0])\n",
        "\n",
        "grid_size = (7, 7)  # last feature map size Resnet-34\n",
        "stride = W // grid_size[0]  # Stride based on image size // grid_size\n",
        "\n",
        "# Generate anchors\n",
        "anchors = anchor_generator.generate_anchors(grid_size, stride)\n",
        "print(anchors.shape)  # Should be (num_anchors, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "o5-U7fK_8RxV",
        "outputId": "441f3b7f-1be7-40e2-cf4f-d9732d51e7cc"
      },
      "outputs": [],
      "source": [
        "# draw box in original image\n",
        "image_with_boxes = image.copy()\n",
        "for (x0, y0, x1, y1) in anchors:\n",
        "    x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
        "    cv2.rectangle(image_with_boxes, (x0, y0), (x1, y1), (0, 255, 0), 1)\n",
        "\n",
        "# show image with bounding box\n",
        "imshow(image_with_boxes, isBGR=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuxMVTIZ6IYD"
      },
      "source": [
        "- Apply Region Proposal Network<br>\n",
        "<img src=\"resource/Region-Proposal-network.png\" width=\"95%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LEGx3Alr6IYF"
      },
      "outputs": [],
      "source": [
        "class SimpleRegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, anchor_generator, rpn_head, nms_thresh=0.4, objectness_thresh=0.3, num_proposals=1000):\n",
        "        super(SimpleRegionProposalNetwork, self).__init__()\n",
        "        self.anchor_generator = anchor_generator\n",
        "        self.rpn_head = rpn_head\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.objectness_thresh = objectness_thresh\n",
        "        self.num_proposals = num_proposals\n",
        "\n",
        "    def forward(self, feature_map, image_size):\n",
        "        # 1. Generate anchors\n",
        "        grid_size = feature_map.shape[-2:]  # (height, width)\n",
        "        stride = image_size[0] // grid_size[0]  # Calculate stride based on input and feature map sizes\n",
        "        anchors = self.anchor_generator.generate_anchors(grid_size, stride)\n",
        "\n",
        "        # 2. Get objectness and bbox deltas from RPN head\n",
        "        objectness, bbox_deltas = self.rpn_head(feature_map)\n",
        "\n",
        "        # 3. Anchor box relative adjustment by bbox deltas to produce region proposals\n",
        "        proposals = self.refine_box(anchors, bbox_deltas)\n",
        "\n",
        "        # 4. Filter proposals using NMS\n",
        "        batch_size = feature_map.shape[0]\n",
        "        final_proposals = []\n",
        "        for i in range(batch_size):\n",
        "            # Select proposals above an objectness threshold for simplicity\n",
        "            keep = objectness[i, :, 0] > self.objectness_thresh\n",
        "            scores = objectness[i, keep, 0]\n",
        "            proposals_for_image = proposals[i][keep]\n",
        "\n",
        "            # Apply Non-Maximum Suppression (NMS)\n",
        "            keep_indices = self.nms(proposals_for_image, scores, self.nms_thresh)\n",
        "            proposals_after_nms = proposals_for_image[keep_indices]\n",
        "\n",
        "            # Limit to the top `num_proposals` proposals\n",
        "            final_proposals.append(proposals_after_nms[:self.num_proposals])\n",
        "\n",
        "        return final_proposals\n",
        "\n",
        "    def refine_box(self, anchors, deltas):\n",
        "        anchors = anchors.unsqueeze(0)  # Add batch dimension\n",
        "        widths = anchors[:, :, 2] - anchors[:, :, 0]\n",
        "        heights = anchors[:, :, 3] - anchors[:, :, 1]\n",
        "        ctr_x = anchors[:, :, 0] + 0.5 * widths\n",
        "        ctr_y = anchors[:, :, 1] + 0.5 * heights\n",
        "\n",
        "        dx = deltas[..., 0]\n",
        "        dy = deltas[..., 1]\n",
        "        dw = deltas[..., 2]\n",
        "        dh = deltas[..., 3]\n",
        "\n",
        "        pred_ctr_x = ctr_x + dx * widths\n",
        "        pred_ctr_y = ctr_y + dy * heights\n",
        "        pred_w = torch.exp(dw) * widths\n",
        "        pred_h = torch.exp(dh) * heights\n",
        "\n",
        "        pred_boxes = torch.zeros_like(deltas)\n",
        "        pred_boxes[..., 0] = pred_ctr_x - 0.5 * pred_w\n",
        "        pred_boxes[..., 1] = pred_ctr_y - 0.5 * pred_h\n",
        "        pred_boxes[..., 2] = pred_ctr_x + 0.5 * pred_w\n",
        "        pred_boxes[..., 3] = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "        return pred_boxes\n",
        "\n",
        "    def nms(self, boxes, scores, iou_threshold):\n",
        "        keep = []\n",
        "        indices = scores.argsort(descending=True)\n",
        "        while indices.numel() > 0:\n",
        "            current = indices[0]\n",
        "            keep.append(current)\n",
        "            if indices.numel() == 1:\n",
        "                break\n",
        "            ious = self.iou(boxes[current].unsqueeze(0), boxes[indices[1:]])\n",
        "            indices = indices[1:][ious.squeeze(0) <= iou_threshold]\n",
        "        return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "    def iou(self, box1, box2):\n",
        "        inter_xmin = torch.max(box1[:, None, 0], box2[:, 0])\n",
        "        inter_ymin = torch.max(box1[:, None, 1], box2[:, 1])\n",
        "        inter_xmax = torch.min(box1[:, None, 2], box2[:, 2])\n",
        "        inter_ymax = torch.min(box1[:, None, 3], box2[:, 3])\n",
        "\n",
        "        inter_area = (inter_xmax - inter_xmin).clamp(min=0) * (inter_ymax - inter_ymin).clamp(min=0)\n",
        "        box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
        "        box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
        "\n",
        "        union_area = box1_area[:, None] + box2_area - inter_area\n",
        "        return inter_area / union_area\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-cdI9GT-VnQ",
        "outputId": "b2585ae4-19b1-4bcc-8267-0615e2232cd9"
      },
      "outputs": [],
      "source": [
        "# Initialize with simple anchor generator\n",
        "anchor_sizes = [32, 64, 128]\n",
        "anchor_aspect_ratios = [0.5, 1.0, 2.0]\n",
        "anchor_generator = SimpleAnchorGenerator(sizes=anchor_sizes, aspect_ratios=anchor_aspect_ratios)\n",
        "\n",
        "# Initialize Simple RPN Head\n",
        "rpn_head = SimpleRPNHead(in_channels=512, num_anchors=len(anchor_sizes) * len(anchor_aspect_ratios))\n",
        "\n",
        "# Initialize Region Proposal Network\n",
        "rpn = SimpleRegionProposalNetwork(anchor_generator, rpn_head, nms_thresh=0.8, objectness_thresh=0.3)\n",
        "\n",
        "# Generate proposals for a feature map Resnet-34\n",
        "proposals = rpn(feature_map, (W, H))\n",
        "print([p.shape for p in proposals])  # Print the shape of proposals for each image in the batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "-xvyXmId8TQU",
        "outputId": "97abad60-694b-4bc5-941f-387344918cc1"
      },
      "outputs": [],
      "source": [
        "# Extract proposals from output tensor\n",
        "proposals_np = proposals[0].detach().cpu().numpy()\n",
        "\n",
        "# Draw bounding boxes on the original image\n",
        "image_with_boxes = image.copy()\n",
        "for (x_min, y_min, x_max, y_max) in proposals_np[:100]:  # Display the top 100 proposals\n",
        "    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
        "    cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), (0, 255, 0), 1)\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "imshow(image_with_boxes, isBGR=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdV_eloC6IYH"
      },
      "source": [
        "_____\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## Source\n",
        "- https://www.linkedin.com/pulse/basic-building-blocks-k-means-clustering-algorithms-hemant-thapa-jnide/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
