{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE2JHng4IBC0"
   },
   "source": [
    "# Auto Annotation using Grounding DINO + Autodistill\n",
    "\n",
    "- Autodistill :\n",
    "    - Autodistill uses *big, slower foundation models* to train small, faster supervised models. \n",
    "    - Using autodistill, you can go from *unlabeled images* to inference with no human intervention in between.\n",
    "    - As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. \n",
    "    <img src=\"resource/autodistil.jpg\" width=\"800px\">\n",
    "    - More about Autodistill Usage (https://docs.autodistill.com/quickstart/#step-5-label-a-dataset)\n",
    "- Grounding DINO : \n",
    "    - Grounding DINO is a `self-supervised learning` algorithm that combines **DINO** (*DETR with Improved deNoising anchor boxes*) with **GLIP** (*Grounded Language-Image Pre-training*). \n",
    "    - Grounding DINO linking textual descriptions (*performed by GLIP*) to their respective visual representations (*performed by DINO*).\n",
    "    - Grounding DINO can detect `arbitrary objects` with `human inputs` such as `category names` or `referring expressions`. \n",
    "    <img src=\"resource/grounding-dino.png\" width=\"800px\"><br><br>\n",
    "    <img src=\"resource/model_explan1.png\" width=\"800px\"><br><br>\n",
    "    <img src=\"resource/model_explan2.png\" width=\"800px\"><br><br>\n",
    "    - More about Grounding DINO (https://github.com/IDEA-Research/GroundingDINO)\n",
    "    - DINO Paper (https://arxiv.org/pdf/2203.03605)\n",
    "    - GLIP Paper (https://arxiv.org/pdf/2112.03857)\n",
    "    - Grounding DINO Paper (https://arxiv.org/abs/2303.05499)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will do\n",
    "- Setting GPU Environment\n",
    "- Installing Autodistill & Grounding DINO\n",
    "- Download Dataset from Google Drive Shared Link (`Dataset Scissors.zip`)\n",
    "- Perform Automatic Annotation using Autodistill & Grounding DINO\n",
    "- Upload Dataset & Annotation to Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Object-Detection-Yolo-OpenCV-Learn/blob/main/Pertemuan%204/4.1 auto-annotation-grounding-dino-autodistill.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Connect GPU Environment \n",
    "\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLRz7KlO2eMv"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Installing Autodistill & Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zp_iQUiIflc"
   },
   "outputs": [],
   "source": [
    "!pip install roboflow autodistill autodistill-grounding-dino supervision -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOQ5RSYTKDBy"
   },
   "source": [
    "## 3.3 Download Dataset from Google Drive\n",
    "- We will use sample dataset [Dataset Scissors.zip](https://drive.google.com/file/d/1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi/view) from GDrive.\n",
    "- If you want to use **your own dataset**, just upload dataset to GDrive and share as **public link** in **ZIP** format.<br>\n",
    "<img src=\"resource/gd-share.png\" width=\"400px\">\n",
    "- Open the shared link in browser, and copy the `GDrive ID` in browser address bar.<br>\n",
    "<img src=\"resource/gd-id.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paste the GDrive ID as value `gdrive_id` variable below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53tUtWm5FsMC"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "DATASET_NAME = 'Scissors_Dataset'\n",
    "\n",
    "# default using gdrive_id Dataset `Scissors.zip` (1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi)\n",
    "gdrive_id = '1PFUlHCwxHAz-zMpP7UD-L47dRAzbrqWi' # <-----  ⚠️⚠️⚠️ USE YOUR OWN GDrive ID ⚠️⚠️⚠️\n",
    "\n",
    "# download zip from GDrive\n",
    "url = 'https://drive.google.com/uc?id=' + gdrive_id\n",
    "gdown.download(url, DATASET_NAME + \".zip\", quiet=False)\n",
    "\n",
    "# unzip dataset\n",
    "!unzip -j {DATASET_NAME}.zip -d {DATASET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bjXknigMgqF"
   },
   "source": [
    "## 3.4 Automatic Annotation using Autodistill & Grounding DINO\n",
    "\n",
    "- Below, we import the dependencies required to running Autodistill & Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3X_nnrUIb-Z"
   },
   "outputs": [],
   "source": [
    "from autodistill_grounding_dino import GroundingDINO\n",
    "from autodistill.detection import CaptionOntology\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW_YGLv1Mm3Q"
   },
   "source": [
    "#### 3.4.1 Label Images with Grounding DINO\n",
    "\n",
    "- Below, we set an *ontology*. Ontologies map `prompts` -- *text given to a model for use in labelling data* -- to the `labels` you want your dataset to include.\n",
    "- For example, the ontology `\"red color apple\": \"apple\"` will send the prompt `\"red color apple\"` to the base model (in this example Grounding DINO). All objects matching that prompt will be labelled `\"apple\"`.\n",
    "- Example if we have multiple object to detect (`apple`, `bottle`, `cap`): \n",
    "    ```\n",
    "    {\n",
    "        \"red color apple\": \"apple\"\n",
    "        \"milk bottle\": \"bottle\",\n",
    "        \"blue cap\": \"cap\"\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMS_detection(predictions):\n",
    "  # Step 1: filter only predistion results with confidence > 0.5 (50%)\n",
    "  # you can change the threshold to other number preferred, e.g (0.75 for confidence above 75%)\n",
    "  predictions = predictions[predictions.confidence > 0.5]\n",
    "\n",
    "  # Step 2: Perform Non-Maximum Suppression\n",
    "  boxes = torch.tensor(predictions.xyxy)  # Convert to PyTorch tensor\n",
    "  scores = torch.tensor(predictions.confidence)\n",
    "  indices = torch.ops.torchvision.nms(boxes, scores, iou_threshold=0.5)  # Adjust IoU threshold as needed\n",
    "\n",
    "  # Step 3: Filter results based on NMS indices\n",
    "  predictions.xyxy = predictions.xyxy[indices.numpy()]\n",
    "  predictions.confidence = predictions.confidence[indices.numpy()]\n",
    "  predictions.class_id = predictions.class_id[indices.numpy()]\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VXD6dU5J3oR"
   },
   "outputs": [],
   "source": [
    "DATASET_FOLDER = \"./\" + DATASET_NAME\n",
    "\n",
    "# create ontology for `Dataset_Scissors`\n",
    "# since we only want to detect `scissors` in dataset,\n",
    "# just define prompt `scissors` and label `scissors`\n",
    "#\n",
    "ONTOLOGY = {\n",
    "    \"scissors\": \"scissors\"\n",
    "}\n",
    "\n",
    "# instantiate base_model using Grounding_DINO\n",
    "base_model = GroundingDINO(ontology=CaptionOntology(ONTOLOGY))\n",
    "\n",
    "images = {}\n",
    "annotations = {}\n",
    "\n",
    "image_names = os.listdir(DATASET_FOLDER)\n",
    "\n",
    "for image_name in image_names:\n",
    "\n",
    "  # find full path of the image /path/to/file.jpg\n",
    "  image_name = os.path.join(DATASET_FOLDER, image_name)\n",
    "\n",
    "  # filter only image data in dataset folder\n",
    "  if not image_name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "    print(image_name)\n",
    "    continue\n",
    "\n",
    "  # perform automatic annotation (detect bounding box & label) from image in dataset folder\n",
    "  predictions = base_model.predict(image_name)\n",
    "\n",
    "  # perform NMS on detection result\n",
    "  predictions = NMS_detection(predictions)\n",
    "\n",
    "  # read image using OpenCV\n",
    "  image = cv2.imread(image_name)\n",
    "\n",
    "  # save the result into `annotations` and `images` dictionary\n",
    "  annotations[image_name] = predictions\n",
    "  images[image_name] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrCTCytRK1RG"
   },
   "source": [
    "- Check detected boundingbox result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0jaTjNV6bhp"
   },
   "outputs": [],
   "source": [
    "for image_name in image_names[:2]: # [:2] --> get only 2 sample image for checking the box result\n",
    "  image_name = os.path.join(DATASET_FOLDER, image_name)\n",
    "\n",
    "  image = images[image_name]\n",
    "  detections = annotations[image_name]\n",
    "\n",
    "  box_annotator = sv.BoxAnnotator(thickness=20)\n",
    "  annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "\n",
    "  %matplotlib inline\n",
    "  sv.plot_image(annotated_image, (5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Upload Generated Annotation and Image Dataset to Roboflow\n",
    "\n",
    "\n",
    "- Convert and save `annotations` data into `Pascal VOC` in `.xml` format\n",
    "- The format is required by Roboflow upload API\n",
    "- More abount `Pascal VOC `Format (http://host.robots.ox.ac.uk/pascal/VOC/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kysgTuR12U-d"
   },
   "outputs": [],
   "source": [
    "ANNOTATIONS_DIRECTORY = os.path.join(HOME, 'annotations')\n",
    "\n",
    "MIN_IMAGE_AREA_PERCENTAGE = 0.002\n",
    "MAX_IMAGE_AREA_PERCENTAGE = 0.80\n",
    "APPROXIMATION_PERCENTAGE = 0.75\n",
    "\n",
    "# extract class names from ONTOLOGY defined on section 3.4.1 above\n",
    "CLASSES = list(ONTOLOGY.values())\n",
    "\n",
    "sv.DetectionDataset(\n",
    "    classes=CLASSES,\n",
    "    images=images,\n",
    "    annotations=annotations\n",
    ").as_pascal_voc(\n",
    "    annotations_directory_path=ANNOTATIONS_DIRECTORY,\n",
    "    min_image_area_percentage=MIN_IMAGE_AREA_PERCENTAGE,\n",
    "    max_image_area_percentage=MAX_IMAGE_AREA_PERCENTAGE,\n",
    "    approximation_percentage=APPROXIMATION_PERCENTAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create **Roboflow Project** automatically using bellow script\n",
    "- The upload proceess will prompting you with `Roboflow Auth Token`,<br>\n",
    "<img src=\"resource/rb_login.png\" width=\"500px\"><br><br>\n",
    "<img src=\"resource/rb-copy-token.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVvMhu5R_B-7"
   },
   "outputs": [],
   "source": [
    "import roboflow\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# define new project name in Roboflow\n",
    "PROJECT_NAME = \"scissors-auto-annotate-1\"\n",
    "PROJECT_DESCRIPTION = \"scissors-auto-annotate-1\"\n",
    "\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "workspace = Roboflow().workspace()\n",
    "new_project = workspace.create_project(\n",
    "    project_name=PROJECT_NAME,\n",
    "    project_license=\"MIT\",\n",
    "    project_type=\"object-detection\",\n",
    "    annotation=PROJECT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upload `Image Dataset` (.jpg) and `Pascal VOC` (.xml) to newly created Roboflow Project above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EnrJVmG_S_q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "image_paths = sv.list_files_with_extensions(directory=DATASET_FOLDER, extensions=[\"jpg\", \"jpeg\", \"png\"])\n",
    "for image_path in tqdm(image_paths):\n",
    "    image_name = image_path.name\n",
    "    annotation_name = f\"{image_path.stem}.xml\"\n",
    "    image_path = str(image_path)\n",
    "    annotation_path = os.path.join(ANNOTATIONS_DIRECTORY, annotation_name)\n",
    "    new_project.upload(\n",
    "        image_path=image_path,\n",
    "        annotation_path=annotation_path,\n",
    "        split=\"train\",\n",
    "        is_prediction=True,\n",
    "        overwrite=True,\n",
    "        tag_names=[\"scissors-auto-annotate\"],\n",
    "        batch_name=\"scissors-auto-annotate\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the uploaded dataset in Roboflow \n",
    "- Open Tab `Annotate`, click on unassigned dataset below<br>\n",
    "<img src=\"resource/rb-auto-uploaded.png\" width=\"800px\">\n",
    "- It will showing all uploaded image data with bounding box\n",
    "- Just click on `Anotate Images` button in top right corner<br>\n",
    "<img src=\"resource/rb-auto-annotated.png\" width=\"800px\">\n",
    "- After that, just click `Manual Labeling` sidebar then click `Assign to Myself`<br>\n",
    "<img src=\"resource/rb-manual-labeling.png\" width=\"300px\"><br>\n",
    "    - <i>Don't worrie, all data is already annotated, so nolonger need to manually annotated the data</i><br><br>\n",
    "- Click on `Add xx images to Dataset` button in the top right corner<br>\n",
    "<img src=\"resource/rb-add-dataset.png\" width=\"800px\">\n",
    "- Then choose method `Split Images Between Train/Valid/Test` and click `Add images` button <br>\n",
    "<img src=\"resource/rb-train-test-split.png\" width=\"300px\">\n",
    "- Completing the process like in `Pertemuan 4` to add `Preprocessing` and `Augmentation` on dataset until the `Create` process.\n",
    "<img src=\"resource/rb-finalized.png\" width=\"600px\">\n",
    "- Now the dataset ready to use, just click on `Download Dataset` button then choose the format to use like usual<br>\n",
    "<img src=\"resource/rb-export.png\" width=\"800px\"><br><br>\n",
    "- Use that Roboflow Dataset in notebook `5.1 train-yolov8-object-detection-on-custom-dataset.ipynb` by your self<br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Object-Detection-Yolo-OpenCV-Learn/blob/main/Pertemuan%204/4.2 train-yolov8-object-detection-on-custom-dataset.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open `5.1 train-yolov8-object-detection-on-custom-dataset.ipynb` In Colab\"/></a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
